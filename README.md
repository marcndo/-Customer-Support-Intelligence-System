# Customer-Support-Intelligence-System

Machine Learning Engineering Portfolio

Marcel Ambo Ndowah — Machine Learning Engineer

This repository presents a focused portfolio of applied machine learning work. It is designed for hiring managers and technical reviewers who want to understand how I approach problems, structure work, and make decisions as a machine learning engineer.

Reviewers should assess this repository as they would a real project handoff: by scanning structure first, then diving into artefacts to evaluate clarity, correctness, and engineering judgment.

## About

I am targeting a Machine Learning Engineer role at the junior to mid level, working on applied ML systems rather than isolated experiments.

My approach emphasizes:

Clear problem framing before modeling

Reproducible and inspectable workflows

Practical trade-offs between accuracy, complexity, and maintainability

Documentation that supports review, debugging, and iteration

I focus on building systems that can be understood, evaluated, and extended by other engineers. I value correctness, traceability, and alignment with real product or business constraints.

## Portfolio Overview

This repository contains end-to-end ML work organized as discrete, reviewable artefacts:

Problem briefs — define scope, assumptions, success criteria

Data pipelines — data ingestion, validation, and preprocessing

Exploratory analyses — targeted analysis to inform modeling choices

Models — baseline and improved models with documented rationale

Evaluation reports — metrics, errors, and limitations

Operational scripts — training, evaluation, and inference entry points

Each section exists to show how decisions are made, not just final results. A reviewer can trace outputs back to inputs, assumptions, and code.

## Core Work Artefacts
Repository Structure
├── problems/
│   └── problem_statement.md
├── data/
│   ├── raw/
│   ├── processed/
│   └── data_validation.md
├── notebooks/
│   ├── exploration.ipynb
│   └── error_analysis.ipynb
├── src/
│   ├── data/
│   ├── features/
│   ├── models/
│   └── evaluation/
├── reports/
│   └── model_evaluation.md
├── scripts/
│   ├── train.py
│   ├── evaluate.py
│   └── infer.py
└── README.md

## What These Artefacts Show

Problem statements clarify scope and constraints before implementation.

Data artefacts demonstrate handling of real, imperfect datasets.

Notebooks are used for analysis, not production logic.

Source code follows modular structure suitable for scaling.

Reports communicate results and limitations clearly.

Scripts reflect how models would be trained or evaluated in practice.

This structure mirrors industry workflows where experimentation, modeling, and evaluation are separated but connected.

Issue Handling and Review Approach

I treat gaps and risks as first-class outputs.

Issues are identified during data inspection, training, or evaluation.

Findings are logged in markdown reports or notebook cells with context.

Each issue includes:

Observed behavior

Likely cause

Impact on metrics or usability

Recommended next step

I prioritize issues based on downstream impact, not theoretical interest. This allows informed decisions about whether to iterate, simplify, or stop.

Technical and Workflow Depth

Projects follow a consistent end-to-end workflow:

Define the problem and success metrics

Inspect and validate data assumptions

Establish a simple baseline model

Iterate with controlled improvements

Evaluate performance and failure modes

Document outcomes and limitations

Tools are connected through scripts rather than manual steps. Data flows from raw inputs to processed datasets, into models, and out to evaluation reports. This structure supports repeatability and review.

Tools and Skills
Skills Demonstrated

Problem framing and metric selection

Data preprocessing and validation

Feature engineering

Supervised learning workflows

Model evaluation and error analysis

Reproducible experimentation

Tools Used in This Repository

Python — core implementation language

NumPy / pandas — data manipulation

scikit-learn — modeling and evaluation

Jupyter Notebook — controlled analysis

Git — version control and review history

Each tool appears only where it serves a clear purpose in the workflow.

Professional Mindset

I approach machine learning as an engineering discipline.

I optimize for clarity before complexity.

I validate assumptions early to reduce downstream risk.

I treat metrics as indicators, not goals.

I document decisions so others can review or challenge them.

My work prioritizes correctness, transparency, and maintainability over novelty.

Contact

GitHub: [https://github.com/your-github-username](https://github.com/marcndo)

Email: ndowahmarcel@gmail.com

Location: Cameroon (open to remote opportunities)

I am interested in junior to mid-level Machine Learning Engineer roles focused on applied ML systems, data-driven products, or AI-enabled services.
